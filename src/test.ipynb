{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "import gensim.models as Fasttext  # For FastText embeddings\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:\\Mini Project\\minipj\\data\\Suicide_Detection.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"class\"], test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000)  # Adjust max vocabulary size as needed\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100  # Adjust maximum post length as needed\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding=\"post\")\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "from urllib.request import urlopen\n",
    "\n",
    "fasttext_model = fasttext.load_model(\"D:\\Mini Project\\minipj\\models\\cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000  # Adjust to match your vocabulary size\n",
    "embedding_dim = 300  # Dimensions of the FastText model\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < vocab_size:\n",
    "        embedding_vector = fasttext_model.get_word_vector(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5800/5802 [============================>.] - ETA: 0s - loss: 0.2516 - accuracy: 0.9005WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001BBB2075CF0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function Model.make_test_function.<locals>.test_function at 0x000001BBB2075CF0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001BBB2075CF0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function Model.make_test_function.<locals>.test_function at 0x000001BBB2075CF0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "5802/5802 [==============================] - 114s 20ms/step - loss: 0.2516 - accuracy: 0.9005 - val_loss: 0.2332 - val_accuracy: 0.9111\n",
      "Epoch 2/10\n",
      "5802/5802 [==============================] - 114s 20ms/step - loss: 0.2069 - accuracy: 0.9200 - val_loss: 0.2084 - val_accuracy: 0.9226\n",
      "Epoch 3/10\n",
      "5802/5802 [==============================] - 111s 19ms/step - loss: 0.1848 - accuracy: 0.9296 - val_loss: 0.2111 - val_accuracy: 0.9196\n",
      "Epoch 4/10\n",
      "5802/5802 [==============================] - 120s 21ms/step - loss: 0.1622 - accuracy: 0.9382 - val_loss: 0.2124 - val_accuracy: 0.9238\n",
      "Epoch 5/10\n",
      "5802/5802 [==============================] - 111s 19ms/step - loss: 0.1387 - accuracy: 0.9465 - val_loss: 0.2326 - val_accuracy: 0.9196\n",
      "Epoch 6/10\n",
      "5802/5802 [==============================] - 126s 22ms/step - loss: 0.1137 - accuracy: 0.9566 - val_loss: 0.2773 - val_accuracy: 0.9181\n",
      "Epoch 7/10\n",
      "5802/5802 [==============================] - 136s 23ms/step - loss: 0.0917 - accuracy: 0.9651 - val_loss: 0.3247 - val_accuracy: 0.9157\n",
      "Epoch 8/10\n",
      "5802/5802 [==============================] - 127s 22ms/step - loss: 0.0730 - accuracy: 0.9720 - val_loss: 0.3847 - val_accuracy: 0.9084\n",
      "Epoch 9/10\n",
      "5802/5802 [==============================] - 117s 20ms/step - loss: 0.0610 - accuracy: 0.9768 - val_loss: 0.4005 - val_accuracy: 0.9087\n",
      "Epoch 10/10\n",
      "5802/5802 [==============================] - 114s 20ms/step - loss: 0.0496 - accuracy: 0.9811 - val_loss: 0.5113 - val_accuracy: 0.9118\n",
      "1451/1451 [==============================] - 8s 6ms/step - loss: 0.5113 - accuracy: 0.9118\n",
      "Test Accuracy: 0.911774218082428\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model architecture\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embedding_matrix], trainable=False),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Single output neuron for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the prepared data\n",
    "model.fit(X_train_padded, y_train, epochs=10, batch_size=32, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After model training:\n",
    "model.save('D:\\Mini Project\\minipj\\models\\CNN_model1.h5')  # Replace with your desired path and filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
